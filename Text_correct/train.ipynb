{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import PinyinMaskedLMDataset\n",
    "from transformers import BertTokenizer\n",
    "from model import PinyinBertForMaskedLM\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../chinese-bert-wwm/ were not used when initializing PinyinBertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing PinyinBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PinyinBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PinyinBertForMaskedLM were not initialized from the model checkpoint at ../chinese-bert-wwm/ and are newly initialized: ['bert.embeddings.pinyin_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_634809/2602638093.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../chinese-bert-wwm/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# model = BertForMaskedLM.from_pretrained(\"../chinese-bert-wwm/\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPinyinBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../chinese-bert-wwm/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPinyinMaskedLMDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../SIGHAN/train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/had/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1809\u001b[0m             )\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/had/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/had/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/had/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/had/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/had/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/had/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/had/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"../chinese-bert-wwm/\")\n",
    "# model = BertForMaskedLM.from_pretrained(\"../chinese-bert-wwm/\")\n",
    "model = PinyinBertForMaskedLM.from_pretrained(\"../chinese-bert-wwm/\").to(device)\n",
    "train_dataset = PinyinMaskedLMDataset(\"../SIGHAN/train.txt\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7870/7870 [23:32<00:00,  5.57it/s, loss=6.38]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "with open(\"../chinese-bert-wwm/pinyin_map.json\") as f:\n",
    "    pinyin_map = json.load(f)\n",
    "\n",
    "total = len(train_dataloader)\n",
    "with tqdm(total=total) as progress:\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
    "        labels = tokenizer(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)[\"input_ids\"].to(device)\n",
    "        pinyin_ids = labels.clone()\n",
    "        for i in range(pinyin_ids.size(0)):\n",
    "            for j in range(pinyin_ids.size(1)):\n",
    "                pinyin_ids[i][j] = pinyin_map[str(pinyin_ids[i][j].item())]\n",
    "        output = model(**inputs, pinyin_ids=pinyin_ids)\n",
    "        loss = loss_fn(output[0].permute(0, 2, 1), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[94201382860928,           0,           0,  ...,           0,\n",
       "                  45, 193273528320],\n",
       "        [          0, 140122620008784, 94201295023200,  ..., 94201265439360,\n",
       "         94201265316416, 94201363046208],\n",
       "        [          0,          64,           0,  ..., 140119013064704,\n",
       "         7365418617031190068, 3617566086534488877],\n",
       "        ...,\n",
       "        [7002937342835032064, 7307199746910528123, 2318330554153050739,  ...,          64,\n",
       "                 113, 94201383018048],\n",
       "        [140122620447344, 140119817332976, 140122620447344,  ..., 3616445622929465956,\n",
       "         6067528668143496499, 3976742471101722929],\n",
       "        [          0,         145, 140122627488736,  ..., 94201382936960,\n",
       "         94201356442512, 140116257499856]], device='cuda:3')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"明天是周[MASK]\"\n",
    "label = \"\"\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
    "    pinyin_ids = inputs[\"input_ids\"].clone()\n",
    "    for i in range(pinyin_ids.size(0)):\n",
    "        for j in range(pinyin_ids.size(1)):\n",
    "            pinyin_ids[i][j] = pinyin_map[str(pinyin_ids[i][j].item())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56 白宫发[MASK]人麦克雷兰表示，南韩国会上周压倒性通过派遣三千多名军队赴伊拉克后，[MASK]希曾打电话给卢武铉表达感激之意。\n",
      "47 47 利比亚扬[MASK]核生化[MASK]器发展计画后，获得美国解除对的黎波里当[MASK]实施的大部分制裁措施，作为回报。\n",
      "31 31 自从一九九九年十二月以来，公司在海外已投资六千五百亿日圆。\n",
      "60 60 贝兹在陈述时说：[MASK]可以清楚的看见一个年轻中国男子的尸体，可能是个[MASK]年。我记得里面非常热，[MASK]运水果的冷冻货柜通常很[MASK]。\n",
      "53 53 柯利巴里与其他七名嫌犯今天将出庭，接受法国最高反恐怖活动法官[MASK]审讯，届时他们料将被交付正式的司法调查。\n",
      "40 40 另一方面，[MASK]赛[MASK]天抵达德黑兰，这是他就任阿富汗临时政府总统之后首次[MASK]问伊朗。\n",
      "32 32 来自世界各地的数千通电话，纷纷对罹[MASK][MASK]的家属表示慰问[MASK]支持。\n",
      "17 17 市民提供的相关合同记者兰洋摄。\n",
      "14 14 [MASK]地金服总[MASK]杨晓冬透露。\n",
      "48 48 国务院发言人埃雷利在形容两人[MASK]电话交谈时说：他们[MASK][MASK]了海地的[MASK]展，以及上周末事[MASK]发[MASK]的经过。\n",
      "13 13 以便政府解锁加密通讯。\n",
      "36 36 美国官员说，中国政府今天驱逐在本月十四日被判间谍罪的美国学者李少民。\n",
      "55 55 国防部说，根据代号-3[MASK]海军建军计划，第一艘神盾级驱逐舰将在二八年底前完成，[MASK]二艘二一年，第三[MASK]二一二年。\n",
      "26 26 同时华盛[MASK]呼吁[MASK]西[MASK]二五年以前缔结泛美贸易条约。\n",
      "15 15 无人机的关注度便迅速蹿升。\n",
      "23 23 也有不少人对为[MASK]女读书购房后的生活很满意。\n",
      "33 33 李登辉在一九九四年访问东南亚时，中国曾向相关国家提出强烈抗议。\n",
      "33 33 在政治压力下皮萨无法再从事广播工作，不过他以为纪录片配音为生。\n",
      "30 30 [MASK]十年代对学术自由[MASK]重新强调并提供了各抒己见的文化环境。\n",
      "46 46 阿丹表示，[MASK]批顾问可望于一月十五日抵达，为一九五一年两国共同防御条约中综合条款的一部份。\n",
      "47 47 工人将不分昼夜的先清理十天来积压在码头的货物，再卸下停泊在港外两[MASK]艘船只的数以百吨计货物。\n",
      "14 14 是一只[MASK]种的[MASK]卡约羊驼。\n",
      "60 62 伍兹一开始也来势汹汹，前三洞有一次15呎和一次20呎的推杆博蒂，但接下来[MASK]洞平标准杆，直到第[MASK]洞才再见博蒂，追平佛雷希。\n",
      "43 43 这座警戒森严的光营的指挥官卡瑞可说，战俘已经被押送到他们的个人监牢，过程平安无事。\n",
      "47 47 尽管伊拉克的安全持续[MASK]化，南韩外交通商部长官[MASK]基文誓言按计划继续派遣[MASK]千名部队前往伊拉克。\n",
      "49 49 报告[MASK]，这些努力的结果，在[MASK]常性资金方面获得改善以及生产设施得到强化下，大多数行业[MASK]取得成长。\n",
      "20 20 汉米尔顿本人始终否认使用禁药[MASK][MASK]控。\n",
      "52 54 欧洲莱德杯高球赛理[MASK]会今天宣布，二00六年莱德杯高球赛[MASK]于二00[MASK]年九月二十二日至二十四日在爱尔兰举行。\n",
      "13 13 四是综合效益[MASK]益显现。\n",
      "53 53 国际货币基金会对[MASK]委内瑞拉的人命损失感到[MASK]憾，依据警方的估计，街头冲突共造成十一人死亡及九十五人受伤。\n",
      "52 52 美国二十[MASK]日提出续延最后一年的让步案，[MASK]目前仍不清楚这项提议是否足以化解联合国安理会其他成员的疑虑。\n",
      "25 25 乌克兰夫妻档艾莲[MASK]与鲁[MASK]兰龚查洛夫，获得铜牌。\n"
     ]
    }
   ],
   "source": [
    "# print(inputs['input_ids'].size(), pinyin_ids.size(), labels.size(), i)\n",
    "for s, l ,p in zip(*list(train_dataloader)[i]):\n",
    "    print(len(tokenizer(s)[\"input_ids\"]), len(pinyin_tokenizer(p)[\"input_ids\"]), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked tokens: 星期[MASK]我要上课\n",
      "Predicted tokens: 星期六我要上课\n"
     ]
    }
   ],
   "source": [
    "masked_lm_logits = output[0]\n",
    "predicted_token_ids = torch.argmax(masked_lm_logits, dim=-1)\n",
    "\n",
    "# 将预测的标记转换回文本\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "\n",
    "# 输出最佳预测\n",
    "print(\"Masked tokens:\", \"\".join(input_tokens[1:-1]))\n",
    "print(\"Predicted tokens:\", \"\".join(predicted_tokens[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiyuxin/anaconda3/envs/had/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertConfig, BertEmbeddings\n",
    "from model import PinyinBertEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../chinese-bert-wwm/config.json\") as f:\n",
    "    config = json.load(f)\n",
    "config = BertConfig(**config)\n",
    "embedding = PinyinBertEmbeddings(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7991, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.8109, grad_fn=<NllLossBackward0>),\n",
       " tensor([[[ -9.6381,  -9.3832,  -9.6146,  ...,  -8.2691,  -7.9518,  -8.2145],\n",
       "          [-15.1690, -14.6613, -15.3457,  ..., -13.3527, -16.9765,  -9.8997],\n",
       "          [-17.4650, -18.3399, -17.1545,  ..., -11.7036, -13.2146,  -7.1388],\n",
       "          ...,\n",
       "          [ -8.7113,  -8.6327,  -8.7318,  ...,  -6.4853,  -5.6840,  -3.5734],\n",
       "          [ -8.8180,  -8.9244,  -8.9428,  ...,  -7.5209,  -8.9219,  -4.4119],\n",
       "          [ -8.5750,  -8.7258,  -8.6462,  ...,  -6.6508,  -7.3145,  -3.6594]],\n",
       " \n",
       "         [[-10.0095,  -9.6976, -10.0291,  ...,  -8.7146,  -8.2553,  -8.9444],\n",
       "          [-14.4107, -13.9038, -13.8943,  ..., -13.5255, -13.0657, -10.8762],\n",
       "          [-14.1578, -13.4532, -13.3644,  ..., -13.8097,  -9.4016, -11.7767],\n",
       "          ...,\n",
       "          [ -9.8684, -10.0710,  -9.4653,  ...,  -7.2605,  -5.4559,  -3.9632],\n",
       "          [ -9.6306,  -9.8551,  -9.0933,  ...,  -7.1194,  -5.7564,  -4.2419],\n",
       "          [ -8.8739,  -8.7286,  -8.4401,  ...,  -7.0411,  -7.8326,  -5.3930]],\n",
       " \n",
       "         [[ -9.8979,  -9.5208,  -9.8658,  ...,  -8.8123,  -8.0652,  -8.8972],\n",
       "          [-13.8938, -14.7910, -14.2271,  ..., -15.8694, -13.5666, -11.1140],\n",
       "          [-16.1183, -15.3969, -14.7648,  ..., -13.3526, -11.4237, -12.6252],\n",
       "          ...,\n",
       "          [ -9.1210,  -9.1515,  -8.8755,  ...,  -8.1625,  -7.6235,  -5.8638],\n",
       "          [ -9.0872,  -9.1592,  -8.7468,  ...,  -7.9498,  -7.6832,  -5.5577],\n",
       "          [ -8.7765,  -8.8281,  -8.5009,  ...,  -7.7052,  -8.3351,  -5.1522]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ -9.8380,  -9.4626,  -9.6918,  ...,  -8.1129,  -7.7345,  -8.6220],\n",
       "          [ -9.3723,  -9.3322,  -8.7064,  ...,  -6.3579,  -5.8149,  -8.3024],\n",
       "          [-16.4107, -16.2407, -15.1189,  ...,  -4.9544,  -6.0066,  -9.4145],\n",
       "          ...,\n",
       "          [ -9.6830, -10.0221,  -9.6742,  ...,  -6.7274,  -5.0127,  -4.8588],\n",
       "          [ -8.8637,  -9.0827,  -8.8017,  ...,  -6.0676,  -4.9672,  -3.9825],\n",
       "          [ -8.6489,  -8.7209,  -8.5906,  ...,  -5.9708,  -6.1391,  -3.9584]],\n",
       " \n",
       "         [[ -9.9910,  -9.7121,  -9.8601,  ...,  -8.8137,  -8.4088,  -9.5023],\n",
       "          [-14.7257, -14.2571, -14.0580,  ...,  -8.6691,  -5.6250, -10.8432],\n",
       "          [-15.6307, -14.4694, -15.0542,  ...,  -9.5873,  -9.0749,  -8.1528],\n",
       "          ...,\n",
       "          [-10.0443,  -9.7835,  -9.3967,  ...,  -7.9977,  -7.2885,  -6.5653],\n",
       "          [-11.3752, -11.2391, -10.4587,  ...,  -9.6926,  -8.5327,  -8.7309],\n",
       "          [ -9.9355,  -9.8935,  -9.2326,  ...,  -8.0689,  -7.2161,  -7.0627]],\n",
       " \n",
       "         [[-10.4231, -10.0672, -10.3317,  ...,  -8.7253,  -7.7776,  -8.2125],\n",
       "          [-18.4927, -17.3554, -16.8872,  ..., -11.7276, -12.7784, -16.0329],\n",
       "          [-17.4472, -16.7528, -15.8895,  ...,  -8.5010, -11.9052, -15.3496],\n",
       "          ...,\n",
       "          [ -7.9784,  -8.4187,  -8.4526,  ...,  -4.6043,  -5.3743,  -0.8149],\n",
       "          [ -9.7976, -10.0067,  -9.7307,  ...,  -8.2006,  -6.2630,  -5.8154],\n",
       "          [-11.8945, -12.3644, -12.2583,  ...,  -9.9982,  -7.7989,  -9.6053]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "had",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
