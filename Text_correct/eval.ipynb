{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertForMaskedLM\n",
    "from model import PinyinBertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../pretrained_models/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PinyinBertForMaskedLM were not initialized from the model checkpoint at ../pretrained_models/bert-base-chinese and are newly initialized: ['bert.embeddings.pinyin_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../pretrained_models/bert-base-chinese\")\n",
    "detect_model = BertForTokenClassification.from_pretrained(\"../pretrained_models/bert-base-chinese\", use_safetensors=True).to(device)\n",
    "correct_model = PinyinBertForMaskedLM.from_pretrained(\"../pretrained_models/bert-base-chinese\",  use_safetensors=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_state_dict = torch.load(\"../no_pretrain/detect_model_epoch_1.ckpt\")\n",
    "correct_state_dict = torch.load(\"../no_pretrain/correct_model_epoch_1.ckpt\")\n",
    "# detect_state_dict = torch.load(\"../check_point/2kk_detect_1.ckpt\")\n",
    "# correct_state_dict = torch.load(\"../check_point/2kk_correct_1.ckpt\")\n",
    "detect_model.load_state_dict(detect_state_dict)\n",
    "correct_model.load_state_dict(correct_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pypinyin import lazy_pinyin\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from build.utils import text2token, token2ids, pinyin_similarity, get_chinese_part\n",
    "\n",
    "detect_model.eval()\n",
    "correct_model.eval()\n",
    "pinyin_vocab = {}\n",
    "with open(\"../build/pinyin_vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        pinyin_vocab[line.strip(\"\\n\")] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection import detect\n",
    "\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    error_label = detect(detect_model, tokenizer, [text], 0.67, show_error=False)[0].unsqueeze(-1).to(device)\n",
    "    text_token, pinyin_token = text2token(text, tokenizer)\n",
    "    pinyin_ids = torch.tensor([0] + token2ids(pinyin_token, pinyin_vocab) + [0], device=device)\n",
    "    output = correct_model(**inputs, pinyin_ids=pinyin_ids, error_prob=error_label)\n",
    "\n",
    "    predict_ids_for_output = output[0].argmax(-1)[0]\n",
    "    result = \"\".join(tokenizer.convert_ids_to_tokens(predict_ids_for_output)[1:-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open(\"../SIGHAN2015/test.json\", 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "cd_tp, cd_fp, cd_tn, cd_fn = 0, 0, 0, 0\n",
    "cc_tp, cc_fp, cc_tn, cc_fn = 0, 0, 0, 0\n",
    "sd_tp, sd_fp, sd_tn, sd_fn = 0, 0, 0, 0\n",
    "sc_tp, sc_fp, sc_tn, sc_fn = 0, 0, 0, 0\n",
    "\n",
    "correct_num = 0\n",
    "for d in test_data:\n",
    "    correct_text = ''.join(tokenizer.tokenize(d['correct_text']))\n",
    "    original_text = ''.join(tokenizer.tokenize(d['original_text']))\n",
    "    predict_text = predict(d['original_text'])\n",
    "    correct_array = [char for char in correct_text]\n",
    "    original_array = [char for char in original_text]\n",
    "    predict_array = [char for char in predict_text]\n",
    "    if \"UNK\" not in correct_text and \"UNK\" not in original_text and \"UNK\" not in predict_text:\n",
    "        # print(f\"original_text:{original_text}, correct_text:{correct_text}, predict_text:{predict_text}\")\n",
    "        # Character-level Metrics\n",
    "        for i in range(len(correct_array)):\n",
    "            correct_char = correct_array[i]\n",
    "            original_char = original_array[i]\n",
    "            predict_char = predict_array[i]\n",
    "            # should correct\n",
    "            if original_char != correct_char:\n",
    "                if original_char != predict_char:\n",
    "                    cd_tp += 1\n",
    "                    if predict_char == correct_char:\n",
    "                        cc_tp += 1\n",
    "                    else:\n",
    "                        cc_fn += 1\n",
    "                else: \n",
    "                    cd_fn += 1\n",
    "                    cc_fn += 1\n",
    "            # should not correct\n",
    "            if original_char == correct_char:\n",
    "                if original_char != predict_char:\n",
    "                    cd_fp += 1\n",
    "                    cc_fp += 1\n",
    "                else: \n",
    "                    cd_tn += 1\n",
    "                    cc_tn += 1\n",
    "    # Sentence-level Metrics\n",
    "    flag_tp = 0\n",
    "    # sentence should correct\n",
    "    if d['correct_text'] != d['original_text']:\n",
    "        # sentence no correction\n",
    "        if predict_text == original_text:\n",
    "            sd_fn += 1\n",
    "            flag_tp = 1\n",
    "        else:\n",
    "            for i in range(len(correct_array)):\n",
    "                correct_char = correct_array[i]\n",
    "                original_char = original_array[i]\n",
    "                predict_char = predict_array[i]\n",
    "                # should correct\n",
    "                if original_char != correct_char:\n",
    "                    # no correction\n",
    "                    if original_char == predict_char:\n",
    "                        sd_fn += 1\n",
    "                        flag_tp = 1\n",
    "                        break\n",
    "                # should not correct\n",
    "                if original_char == correct_char:\n",
    "                    if original_char != predict_char:\n",
    "                        sd_fn += 1\n",
    "                        flag_tp = 1\n",
    "                        break        \n",
    "        if flag_tp == 0:\n",
    "            sd_tp += 1\n",
    "\n",
    "    if d['correct_text'] != d['original_text'] and predict_text == correct_text:\n",
    "        sc_tp += 1\n",
    "    elif d['correct_text'] == d['original_text'] and predict_text != correct_text:\n",
    "        sd_fp += 1\n",
    "        sc_fp += 1\n",
    "    elif d['correct_text'] == d['original_text'] and predict_text == correct_text:\n",
    "        sd_tn += 1\n",
    "        sc_tn += 1\n",
    "    elif d['correct_text'] != d['original_text'] and predict_text != correct_text:\n",
    "        sc_fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level Detection Metrics\n",
      "tp:547, fp:136, tn:32574, fn:138\n",
      "accuracy:0.9917951789189998 \n",
      "precision: 0.8008784773060029 \n",
      "recall: 0.7985401459854015 \n",
      "f1: 0.7997076023391813\n",
      "Character-level Detection Metrics\n",
      "tp:382, fp:136, tn:32574, fn:303\n",
      "accuracy:0.9868543195089086 \n",
      "precision: 0.7374517374517374 \n",
      "recall: 0.5576642335766423 \n",
      "f1: 0.6350789692435578\n",
      "Sentence-level Detection Metrics\n",
      "tp:413, fp:98, tn:459, fn:130\n",
      "accuracy:0.79 \n",
      "precision: 0.8070866141732284 \n",
      "recall: 0.7550644567219152 \n",
      "f1: 0.780209324452902\n",
      "Sentence-level Correction Metrics\n",
      "tp:309, fp:98, tn:459, fn:234\n",
      "accuracy:0.6981818181818182 \n",
      "precision: 0.7592137592137592 \n",
      "recall: 0.569060773480663 \n",
      "f1: 0.6505263157894736\n"
     ]
    }
   ],
   "source": [
    "print(\"Character-level Detection Metrics\")\n",
    "accuracy = (cd_tp + cd_tn) / (cd_tp + cd_fn + cd_tn + cd_fp)\n",
    "precision = cd_tp / (cd_tp + cd_fp)\n",
    "recall = cd_tp / (cd_tp + cd_fn)\n",
    "f1 =  2 * precision * recall / (precision + recall)\n",
    "print(f\"tp:{cd_tp}, fp:{cd_fp}, tn:{cd_tn}, fn:{cd_fn}\\naccuracy:{accuracy} \\nprecision: {precision} \\nrecall: {recall} \\nf1: {f1}\")\n",
    "\n",
    "print(\"Character-level Detection Metrics\")\n",
    "accuracy = (cc_tp + cc_tn) / (cc_tp + cc_fn + cc_tn + cc_fp)\n",
    "precision = cc_tp / (cc_tp + cc_fp)\n",
    "recall = cc_tp / (cc_tp + cc_fn)\n",
    "f1 =  2 * precision * recall / (precision + recall)\n",
    "print(f\"tp:{cc_tp}, fp:{cc_fp}, tn:{cc_tn}, fn:{cc_fn}\\naccuracy:{accuracy} \\nprecision: {precision} \\nrecall: {recall} \\nf1: {f1}\")\n",
    "\n",
    "print(\"Sentence-level Detection Metrics\")\n",
    "accuracy = (sd_tp - 3 + sd_tn) / (sd_tp + sd_fn + sd_tn + sd_fp)\n",
    "precision = (sd_tp - 3) / (sd_tp - 3 + sd_fp)\n",
    "recall = (sd_tp - 3) / (sd_tp + sd_fn)\n",
    "f1 =  2 * precision * recall / (precision + recall)\n",
    "print(f\"tp:{sd_tp}, fp:{sd_fp}, tn:{sd_tn}, fn:{sd_fn}\\naccuracy:{accuracy} \\nprecision: {precision} \\nrecall: {recall} \\nf1: {f1}\")\n",
    "\n",
    "print(\"Sentence-level Correction Metrics\")\n",
    "accuracy = (sc_tp + sc_tn) / (sc_tp + sc_fn + sc_tn + sc_fp)\n",
    "precision = sc_tp / (sc_tp + sc_fp)\n",
    "recall = sc_tp / (sc_tp + sc_fn)\n",
    "f1 =  2 * precision * recall / (precision + recall)\n",
    "print(f\"tp:{sc_tp}, fp:{sc_fp}, tn:{sc_tn}, fn:{sc_fn}\\naccuracy:{accuracy} \\nprecision: {precision} \\nrecall: {recall} \\nf1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    text = [text]\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    error_label = detect(detect_model, tokenizer, text, 0.2, show_error=False).unsqueeze(-1).to(device)\n",
    "    text_token, pinyin_token = text2token(text[0], tokenizer)\n",
    "    pinyin_ids = torch.tensor([0] + token2ids(pinyin_token, pinyin_vocab) + [0], device=device)\n",
    "    output = correct_model(**inputs, pinyin_ids=pinyin_ids, error_prob=error_label)\n",
    "    \n",
    "    predict_ids_for_output = output[0].argmax(-1)[0]\n",
    "    for i, l in enumerate(error_label.squeeze()):\n",
    "        if l:\n",
    "            predict_ids = predict_ids_for_output.clone()\n",
    "            max_sim = 0.01\n",
    "            min_sco = 1e6\n",
    "            max_s = 0\n",
    "            for ind in output[0][:, i, :].topk(k=5).indices[0]:\n",
    "                predict_ids[i] = ind\n",
    "                score = detect_model(predict_ids.unsqueeze(0)).logits\n",
    "                score = torch.nn.functional.softmax(score, dim=-1)[:, :, 1]\n",
    "                sim = pinyin_similarity(lazy_pinyin(tokenizer.convert_ids_to_tokens([ind]))[0], \\\n",
    "                                      lazy_pinyin(tokenizer.convert_ids_to_tokens([inputs['input_ids'][0][i]]))[0])\n",
    "                print(f\"{score[0][i].item():.5f} {sim:.2f}, {tokenizer.convert_ids_to_tokens([ind])}\")\n",
    "                if (min_sco - score[0][i] + 0.1) * sim / max_sim > max_s:\n",
    "                    max_sim =  sim\n",
    "                    min_sco = score[0][i]\n",
    "                    max_s = (min_sco - score[0][i] + 0.1) * sim / max_sim\n",
    "                    predict_ids_for_output[i] = ind\n",
    "    text = [\"\".join(tokenizer.convert_ids_to_tokens(predict_ids_for_output)[1:-1])]\n",
    "                    # print(predict_text)\n",
    "    return text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top5(text):\n",
    "    text = [text]\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    error_label = detect(detect_model, tokenizer, text, 0.67, show_error=False)[0].unsqueeze(-1).to(device)\n",
    "    pinyin_ids = torch.tensor([0] + token2ids(text2token(text[0], tokenizer)[1], pinyin_vocab) + [0], device=device)\n",
    "    output = correct_model(**inputs, pinyin_ids=pinyin_ids, error_prob=error_label)\n",
    "    \n",
    "    predict_ids_for_output = output[0].argmax(-1)[0]\n",
    "    for i, l in enumerate(error_label.squeeze()):\n",
    "        if l:\n",
    "            print(tokenizer.convert_ids_to_tokens(output[0][:, i, :].topk(k=5).indices[0]))\n",
    "    \n",
    "    return \"\".join(tokenizer.convert_ids_to_tokens(predict_ids_for_output)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_error_prob(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    outputs = detect_model(**inputs).logits\n",
    "    outputs = torch.nn.functional.softmax(outputs, dim=-1)[:, :, 1]\n",
    "    for i, (a, b) in enumerate(zip(text, outputs[0][1:-1])):\n",
    "        norm_prob = 0\n",
    "        if i == 0:\n",
    "            norm_prob = 0.5*b + 0.5*outputs[0][1:-1][i+1]\n",
    "        elif i == len(text) - 1:\n",
    "            norm_prob = 0.5*b + 0.5*outputs[0][1:-1][i-1]\n",
    "        else:\n",
    "            norm_prob = 0.4*b + 0.3*outputs[0][1:-1][i-1] + 0.3*outputs[0][1:-1][i+1]\n",
    "        print(f\"{a} : {b:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 : 0.003\n",
      "吃 : 0.006\n",
      "早 : 0.087\n",
      "菜 : 0.827\n",
      "了 : 0.002\n",
      "吗 : 0.002\n",
      "？ : 0.000\n",
      "['餐', '饭', '点', '起', '盘']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你吃早餐了吗？'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"你吃早菜了吗？\"\n",
    "token_error_prob(text)\n",
    "predict_top5(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我跟小明在客厅边喝啤酒跟看电视，小明的流量很好，喝了很多可是没有醉。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"我跟小明在客厅便喝啤酒便看电视，小明的流量很好，喝了很多可是没有醉。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
