{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "pretrained = \"../t5_corrector_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
    "model = T5ForConditionalGeneration.from_pretrained(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下个星期,我跟我朋唷打算去法国玩儿。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def correct(text, max_length):\n",
    "    model_inputs = tokenizer(text, \n",
    "                                max_length=max_length, \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**model_inputs, \n",
    "                              num_beams=5,\n",
    "                              no_repeat_ngram_size=4,\n",
    "                              do_sample=True, \n",
    "                              early_stopping=True,\n",
    "                              max_length=max_length,\n",
    "                              return_dict_in_generate=True,\n",
    "                              output_scores=True)\n",
    "    pred_output = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)[0]\n",
    "    return pred_output\n",
    "\n",
    "text = \"下个星期，我跟我朋唷打算去法国玩儿。\"\n",
    "correction = correct(text, max_length=32)\n",
    "print(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-08 21:03:54.693\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpycorrector.t5.t5_corrector\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mUse device: cuda\u001b[0m\n",
      "\u001b[32m2023-11-08 21:03:54.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpycorrector.t5.t5_corrector\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mLoaded t5 correction model: ../t5_corrector_v2, spend: 2.442 s.\u001b[0m\n",
      "\u001b[32m2023-11-08 21:03:57.304\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpycorrector.t5.t5_corrector\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mUse device: cuda\u001b[0m\n",
      "\u001b[32m2023-11-08 21:03:57.305\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpycorrector.t5.t5_corrector\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mLoaded t5 correction model: ../mengzi_t5_base_correction, spend: 2.610 s.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pycorrector.t5.t5_corrector import T5Corrector\n",
    "nlp1 = T5Corrector(\"../t5_corrector_v2\").batch_t5_correct\n",
    "nlp2 = T5Corrector(\"../mengzi_t5_base_correction\").batch_t5_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pypinyin import lazy_pinyin\n",
    "def mask_text(text: str, prob:float=0.1, momentum:float=0.9):\n",
    "    masked_text = \"\"\n",
    "    factor = 0\n",
    "    for c in text:\n",
    "        if random.random() < prob*pow(momentum, factor) and ('\\u4e00' <= c <= '\\u9fff'):\n",
    "            masked_text += \"[MASK]\"\n",
    "            factor += 1\n",
    "        else:\n",
    "            masked_text += c\n",
    "            factor = 0\n",
    "    return masked_text\n",
    "\n",
    "def text2pinyin(text):\n",
    "    pinyin_text = \"\"\n",
    "    for c in text:\n",
    "        if '\\u4e00' <= c <= '\\u9fff':\n",
    "            pinyin_text += lazy_pinyin(c)[0]\n",
    "        else:\n",
    "            pinyin_text += \"[OTHER]\"\n",
    "        pinyin_text += \" \"\n",
    "    return pinyin_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinyin_map = {}\n",
    "with open('../chinese-bert-wwm/pinyin_vocab.txt') as f:\n",
    "    lines = f.readlines()\n",
    "for i, line in enumerate(lines):\n",
    "    pinyin_map[line.strip('\\n')] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251835/251835 [00:03<00:00, 79988.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open(\"../SIGHAN/train.json\", encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"../SIGHAN/train.txt\", 'a', encoding='utf-8') as f:\n",
    "    for data in tqdm(train_data):\n",
    "        f.write(f\"{mask_text(data['correct_text'], prob=0.05)}\\t{data['correct_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../chinese-bert-wwm/vocab.txt\", encoding=\"utf-8\") as  f:\n",
    "    vocabs = f.readlines()\n",
    "pinyin_idx = {}\n",
    "with open(\"../chinese-bert-wwm/pinyin_vocab.txt\", encoding=\"utf-8\") as f:\n",
    "    pinyin_vocab = f.readlines()\n",
    "    for i, pinyin in enumerate(pinyin_vocab):\n",
    "        pinyin_idx[pinyin.strip(\"\\n\")] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wo']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import lazy_pinyin\n",
    "lazy_pinyin(\"wo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "\n",
      "[CLS]\n",
      "\n",
      "[SEP]\n",
      "\n",
      "[MASK]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pypinyin import lazy_pinyin\n",
    "pinyin_map = {}\n",
    "with open(\"../chinese-bert-wwm/pinyin_map.json\", \"w\") as f:\n",
    "    for i, vocab in enumerate(vocabs):\n",
    "        if pinyin_idx.get(vocab.strip(\"\\n\")) is not None and vocab[0] == '[':\n",
    "            pinyin_map[i] = pinyin_idx[vocab.strip(\"\\n\")]\n",
    "            print(vocab)\n",
    "        elif '\\u4e00' <= vocab.strip(\"\\n\") <= '\\u9fff':\n",
    "            pinyin_map[i] = pinyin_idx[lazy_pinyin(vocab.strip(\"\\n\"))[0]]\n",
    "        else:\n",
    "            pinyin_map[i] = pinyin_idx[\"[OTHER]\"]\n",
    "    json.dump(pinyin_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinyin_idx.get(vocabs[0].strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiyuxin/anaconda3/envs/had/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at ../chinese-bert-wwm/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "model = BertForMaskedLM.from_pretrained('../chinese-bert-wwm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"../chinese-bert-wwm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"目前这次事件的细节还不清楚，[MASK]亡人数也未确定。\"\n",
    "target = \"我下个礼拜已[MASK]搬家到新的地址了，在这里我觉得那么方便，离学校不太远，去超[MASK]市场很近。\"\n",
    "# tokens = tokenizer(text, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    output = model(**tokenizer([text, target], return_tensors='pt', max_length=64, padding=True, truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted tokens: 我下个礼拜已经搬家到新的地址了，在这里我觉得那么方便，离学校不太远，去超级市场很近。\n"
     ]
    }
   ],
   "source": [
    "masked_lm_logits = output.logits\n",
    "predicted_token_ids = torch.argmax(masked_lm_logits, dim=-1)\n",
    "\n",
    "# 将预测的标记转换回文本\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[1])\n",
    "\n",
    "# 输出最佳预测\n",
    "print(\"Predicted tokens:\", \"\".join(predicted_tokens[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../chinese-bert-wwm/pinyin_vocab.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "with open(\"../chinese-bert-wwm/pinyin_vocab.txt\", 'w') as f:\n",
    "    f.writelines([line for i, line in enumerate(lines) if i%5 == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import lazy_pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ni', 'hai', 'qian', 'qian', 'huan', 'xiang', '？']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_pinyin(\"你还欠钱还乡？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "had",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
